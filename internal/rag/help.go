package rag

import "fmt"

// ShowHelp displays the help message
func ShowHelp(maxTokensPerChunk, chunkOverlapPercent, maxContextTokens int) {
	fmt.Println("mcp-markdown-rag - Local Markdown Document Indexing and Semantic Search")
	fmt.Println()
	fmt.Println("Features:")
	fmt.Println("  - Self-contained local embedding (no external services required)")
	fmt.Println("  - Optional Ollama integration for higher quality embeddings")
	fmt.Println("  - Automatic chunking of large files with semantic boundaries")
	fmt.Println("  - Structure-aware splitting at headings and sentence boundaries")
	fmt.Println("  - 15% overlap between chunks for better context preservation")
	fmt.Println("  - MCP server mode for AI assistant integration")
	fmt.Println()
	fmt.Println("Usage:")
	fmt.Println("  -index <path>              Index all .md files in the specified folder recursively")
	fmt.Println("  -query <text>              Search for documents similar to the query text")
	fmt.Println("  -list                      List all documents in the database")
	fmt.Println("  -stats                     Show statistics about the database contents")
	fmt.Println("  -db <path>                 Path to database file (default: ./rag.db)")
	fmt.Println("  -embedding-mode <mode>     Embedding mode: 'local' (default) or 'ollama'")
	fmt.Println("  -ollama-url <url>          Ollama API URL (default: http://localhost:11434/api/embeddings)")
	fmt.Println("  -embedding-model <model>   Embedding model name for ollama mode (default: nomic-embed-text)")
	fmt.Println("  -mcp                       Run as MCP server (enables MCP protocol over stdio)")
	fmt.Println("  -help                      Show this help message")
	fmt.Println()
	fmt.Println("Environment Variables:")
	fmt.Println("  RAG_DB_PATH               Database file path")
	fmt.Println("  RAG_EMBEDDING_MODE        Embedding mode ('local' or 'ollama')")
	fmt.Println("  RAG_OLLAMA_URL            Ollama API URL (only used in ollama mode)")
	fmt.Println("  RAG_EMBEDDING_MODEL       Embedding model name (only used in ollama mode)")
	fmt.Println()
	fmt.Println("Priority: Command line arguments > Environment variables > Defaults")
	fmt.Println()
	fmt.Println("Examples:")
	fmt.Println("  # Index documents using built-in local embeddings (no external services)")
	fmt.Println("  ./mcp-markdown-rag -index /path/to/documents")
	fmt.Println()
	fmt.Println("  # Search indexed documents")
	fmt.Println("  ./mcp-markdown-rag -query \"machine learning concepts\"")
	fmt.Println()
	fmt.Println("  # Run as MCP server")
	fmt.Println("  ./mcp-markdown-rag -mcp")
	fmt.Println()
	fmt.Println("  # Use Ollama for embeddings instead of local")
	fmt.Println("  ./mcp-markdown-rag -embedding-mode ollama -index /path/to/documents")
	fmt.Println()
	fmt.Println("  # List and view statistics")
	fmt.Println("  ./mcp-markdown-rag -list")
	fmt.Println("  ./mcp-markdown-rag -stats")
	fmt.Println()
	fmt.Println("Chunking Configuration:")
	fmt.Printf("  Max tokens per chunk: %d\n", maxTokensPerChunk)
	fmt.Printf("  Chunk overlap: %d%%\n", chunkOverlapPercent)
	fmt.Printf("  Context window limit: %d tokens\n", maxContextTokens)
}
