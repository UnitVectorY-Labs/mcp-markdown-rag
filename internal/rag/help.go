package rag

import "fmt"

// ShowHelp displays the help message
func ShowHelp(maxTokensPerChunk, chunkOverlapPercent, maxContextTokens int) {
	fmt.Println("Local MCP RAG - Document Indexing and Search")
	fmt.Println("Features:")
	fmt.Println("  - Automatic chunking of large files (>1000 tokens) with semantic boundaries")
	fmt.Println("  - Structure-aware splitting at headings and sentence boundaries")
	fmt.Println("  - 15% overlap between chunks for better context preservation")
	fmt.Println("  - Batch embedding processing with retry logic")
	fmt.Println()
	fmt.Println("Usage:")
	fmt.Println("  -index <path>              Index all .md files in the specified folder recursively")
	fmt.Println("  -query <text>              Search for documents similar to the query text")
	fmt.Println("  -list                      List all documents in the database")
	fmt.Println("  -stats                     Show statistics about the database contents")
	fmt.Println("  -db <path>                 Path to database file (default: ./rag.db)")
	fmt.Println("  -ollama-url <url>          Ollama API URL (default: http://localhost:11434/api/embeddings)")
	fmt.Println("  -embedding-model <model>   Embedding model name (default: nomic-embed-text)")
	fmt.Println("  -mcp                       Run as MCP server (enables MCP protocol endpoints)")
	fmt.Println("  -help                      Show this help message")
	fmt.Println()
	fmt.Println("Environment Variables:")
	fmt.Println("  RAG_DB_PATH               Database file path")
	fmt.Println("  RAG_OLLAMA_URL            Ollama API URL")
	fmt.Println("  RAG_EMBEDDING_MODEL       Embedding model name")
	fmt.Println()
	fmt.Println("Priority: Command line arguments > Environment variables > Defaults")
	fmt.Println()
	fmt.Println("Examples:")
	fmt.Println("  ./rag -index /path/to/documents")
	fmt.Println("  ./rag -query \"machine learning concepts\"")
	fmt.Println("  ./rag -list")
	fmt.Println("  ./rag -stats")
	fmt.Println("  ./rag -index ./docs -db /tmp/my-rag.db")
	fmt.Println("  RAG_DB_PATH=/tmp/rag.db ./rag -list")
	fmt.Println()
	fmt.Println("Chunking Configuration:")
	fmt.Printf("  Max tokens per chunk: %d\n", maxTokensPerChunk)
	fmt.Printf("  Chunk overlap: %d%%\n", chunkOverlapPercent)
	fmt.Printf("  Context window limit: %d tokens\n", maxContextTokens)
	fmt.Println()
	fmt.Println("Requirements:")
	fmt.Println("  - Ollama must be running locally on the specified port")
	fmt.Println("  - The embedding model must be available in Ollama")
}
